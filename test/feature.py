'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import os

# ========== ë°ì´í„° ë¡œë“œ ==========
path = 'datasets/training-set.csv'
data = pd.read_csv(path)

print(f"ì›ë³¸ shape: {data.shape}\n")

# ========== ì „ì²˜ë¦¬ ==========
# id ì œê±°
data = data.drop(columns=['id'])

# Categorical ì¸ì½”ë”©
categorical_cols = ['proto', 'service', 'state']
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# ë ˆì´ë¸” ë¶„ë¦¬
y = data['attack_cat'].values
X = data.drop(columns=['attack_cat'])

# í”¼ì²˜ëª… ì €ì¥ (ì¤‘ìš”!)
feature_names = X.columns.tolist()
print(f"ì „ì²´ í”¼ì²˜ ê°œìˆ˜: {len(feature_names)}")
print(f"í”¼ì²˜ ëª©ë¡: {feature_names}\n")

X = X.values

# ========== Random Forestë¡œ Feature Importance ë¶„ì„ ==========
print("=== Feature Importance ë¶„ì„ ì¤‘, ëƒ¥ëƒ¥! ===\n")

rf = RandomForestClassifier(
    n_estimators=100, 
    random_state=42, 
    n_jobs=-1,
    max_depth=10
)

# Labelë„ ì¸ì½”ë”©
le_label = LabelEncoder()
y_encoded = le_label.fit_transform(y)

rf.fit(X, y_encoded)

# ========== ê²°ê³¼ ì •ë¦¬ ==========
importances_df = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("=== Top 20 ì¤‘ìš”í•œ í”¼ì²˜, ëƒ¥! ===")
print(importances_df.head(20).to_string(index=False))

print("\n=== Bottom 10 ëœ ì¤‘ìš”í•œ í”¼ì²˜, ëƒ¥! ===")
print(importances_df.tail(10).to_string(index=False))

# ========== ì‹œê°í™” ==========
plt.figure(figsize=(12, 8))

# ìƒìœ„ 20ê°œ
plt.subplot(1, 2, 1)
top_20 = importances_df.head(20)
plt.barh(range(len(top_20)), top_20['importance'])
plt.yticks(range(len(top_20)), top_20['feature'])
plt.xlabel('Importance')
plt.title('Top 20 Important Features')
plt.gca().invert_yaxis()

# í•˜ìœ„ 15ê°œ
plt.subplot(1, 2, 2)
bottom_15 = importances_df.tail(15)
plt.barh(range(len(bottom_15)), bottom_15['importance'])
plt.yticks(range(len(bottom_15)), bottom_15['feature'])
plt.xlabel('Importance')
plt.title('Bottom 15 Features (Candidates to Drop)')
plt.gca().invert_yaxis()

plt.tight_layout()

# ========== ì œê±° í›„ë³´ ì œì•ˆ ==========
print("\n=== ì œê±° ì¶”ì²œ í”¼ì²˜ (importance < 0.01), ëƒ¥ëƒ¥! ===")
low_importance = importances_df[importances_df['importance'] < 0.01]
print(low_importance.to_string(index=False))
print(f"\nì´ {len(low_importance)}ê°œ í”¼ì²˜ ì œê±° ê³ ë ¤, ëƒ¥!")

print("\në¶„ì„ ì™„ë£Œ, ëƒ¥ëƒ¥! ğŸ‰")


'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# ========== ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ==========
path = 'datasets/training-data-preprocess.csv'
data = pd.read_csv(path)

# id ì œê±°, categorical ì¸ì½”ë”©
# data = data.drop(columns=['id'])
categorical_cols = ['proto', 'service', 'state']
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# í”¼ì²˜ëª… (ì œê±°ëœ 10ê°œ ë¹¼ê³ )
drop_features = [
    'is_ftp_login', 'ct_ftp_cmd', 'dwin', 'dtcpb', 'stcpb',
    'response_body_len', 'ct_flw_http_mthd', 'trans_depth',
    'is_sm_ips_ports', 'swin']

feature_names = [col for col in data.columns 
                 if col not in ['attack_cat']]

print(f"ì‚¬ìš© í”¼ì²˜ ê°œìˆ˜: {len(feature_names)}")
print(f"í”¼ì²˜ ëª©ë¡: {feature_names}\n")

# ========== ê³µê²©ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸ ==========
print("=" * 60)
print("=== ê³µê²© ìœ í˜•ë³„ ìƒ˜í”Œ ë¶„í¬, ëƒ¥! ===")
print("=" * 60)

attack_counts = data['attack_cat'].value_counts()
print(attack_counts)
print(f"\nì „ì²´: {len(data)}ê°œ")
print(f"ê³µê²© ìœ í˜•: {len(attack_counts)}ê°œ\n")

# ========== ì „ì²´ ê³µê²© ìœ í˜• ë¶„ì„ ==========
all_attacks = attack_counts.index.tolist()
normal_count = attack_counts['Normal']

results = {}

for attack in all_attacks:
    if attack == 'Normal':
        continue
        
    print(f"\n{'='*60}")
    print(f"=== {attack} ê³µê²© ë¶„ì„, ëƒ¥! ===")
    print(f"{'='*60}\n")
    
    # í•´ë‹¹ ê³µê²© vs Normal (ì´ì§„ ë¶„ë¥˜)
    attack_mask = data['attack_cat'] == attack
    normal_mask = data['attack_cat'] == 'Normal'
    
    binary_data = data[attack_mask | normal_mask].copy()
    
    attack_count = attack_mask.sum()
    print(f"{attack} ìƒ˜í”Œ ìˆ˜: {attack_count} ({attack_count/len(data)*100:.2f}%)")
    print(f"Normal ìƒ˜í”Œ ìˆ˜: {normal_count}")
    print(f"ë¹„ìœ¨: 1:{normal_count/attack_count:.1f}\n")
    
    # X, y ë¶„ë¦¬ (ì œê±°í•  í”¼ì²˜ ë¹¼ê³ )
    X_cols = [col for col in binary_data.columns 
              if col not in ['attack_cat']]
    X_binary = binary_data[X_cols].values
    y_binary = (binary_data['attack_cat'] == attack).astype(int)
    
    # Random Forest í•™ìŠµ
    rf = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=-1,
        max_depth=10
    )
    rf.fit(X_binary, y_binary)
    
    # Feature Importance
    importances = pd.DataFrame({
        'feature': feature_names,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # ê²°ê³¼ ì €ì¥
    results[attack] = {
        'count': attack_count,
        'ratio': normal_count/attack_count,
        'top_features': importances.head(10),
        'all_importances': importances
    }
    
    print(f"=== {attack} íƒì§€ í•µì‹¬ Top 10 í”¼ì²˜, ëƒ¥! ===")
    print(importances.head(10).to_string(index=False))

# ========== ì „ì²´ ìš”ì•½ ==========
print("\n" + "=" * 60)
print("=== ì „ì²´ ê³µê²© ìœ í˜• ìš”ì•½, ëƒ¥ëƒ¥! ===")
print("=" * 60)

summary = pd.DataFrame({
    'Attack': list(results.keys()),
    'Count': [results[a]['count'] for a in results.keys()],
    'Ratio_to_Normal': [f"1:{results[a]['ratio']:.1f}" for a in results.keys()],
    'Top_Feature': [results[a]['top_features'].iloc[0]['feature'] for a in results.keys()],
    'Top_Importance': [f"{results[a]['top_features'].iloc[0]['importance']:.3f}" for a in results.keys()]
})

print(summary.to_string(index=False))

# ========== ì‹œê°í™” ==========
fig, axes = plt.subplots(3, 3, figsize=(18, 15))
axes = axes.flatten()

for idx, attack in enumerate(results.keys()):
    if idx >= 9:
        break
    
    top_10 = results[attack]['top_features'].head(10)
    
    axes[idx].barh(range(len(top_10)), top_10['importance'])
    axes[idx].set_yticks(range(len(top_10)))
    axes[idx].set_yticklabels(top_10['feature'], fontsize=8)
    axes[idx].set_xlabel('Importance', fontsize=9)
    axes[idx].set_title(f'{attack} (n={results[attack]["count"]})', fontsize=10)
    axes[idx].invert_yaxis()

plt.tight_layout()
plt.show()

print("\nğŸ‰ ì „ì²´ ê³µê²© ìœ í˜• ë¶„ì„ ì™„ë£Œ, ëƒ¥ëƒ¥!")

'''
import pandas as pd

# Test set ë¡œë“œ
test_data = pd.read_csv('datasets/training-data-preprocess.csv')

# Proto ë¶„í¬ í™•ì¸
proto_counts = test_data['proto'].value_counts()

print("=== Test Set Proto ë¶„í¬ ===")
print(proto_counts)
print(f"\nì´ í”„ë¡œí† ì½œ ì¢…ë¥˜: {len(proto_counts)}ê°œ")
print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(test_data)}ê°œ")

# ê° í”„ë¡œí† ì½œ ë¹„ìœ¨
print("\n=== ë¹„ìœ¨ ===")
for proto, count in proto_counts.items():
    percentage = count / len(test_data) * 100
    print(f"{proto}: {count}ê°œ ({percentage:.2f}%)")

# Cleanì— ì—†ë˜ í”„ë¡œí† ì½œ
clean_data = pd.read_csv('datasets/training-data-preprocess.csv')
clean_protos = set(clean_data['proto'].unique())
test_protos = set(test_data['proto'].unique())

missing_protos = test_protos - clean_protos

print(f"\n=== Cleanì— ì—†ëŠ” í”„ë¡œí† ì½œ ===")
print(f"ì¢…ë¥˜: {missing_protos}")

# ê°ê°ì˜ ìƒ˜í”Œ ìˆ˜
print("\nìƒì„¸:")
for proto in missing_protos:
    count = len(test_data[test_data['proto'] == proto])
    percentage = count / len(test_data) * 100
    print(f"{proto}: {count}ê°œ ({percentage:.2f}%)")

total_missing = len(test_data[test_data['proto'].isin(missing_protos)])
print(f"\nì´ ì˜í–¥ë°›ëŠ” ìƒ˜í”Œ: {total_missing}ê°œ ({total_missing/len(test_data)*100:.2f}%)")

'''